### Note: Understanding the Two README Files

This project contains two README files to provide clear and detailed context:

1. **`README_RUN_CODE.md`**: This file contains all the instructions for setting up, running the code, and interpreting the results. If youâ€™re looking to execute the code and analyze its outputs, this is the file you should start with.

2. **`README_THOUGHTS.md`**: This file explains my thought process, design decisions, challenges faced, and alternative approaches considered while solving the coding challenge. It offers deeper insight into the logic and reasoning behind the implementation. 

Feel free to refer to each README based on your purpose: whether you want to run the code or understand the rationale behind it!

## How to Run the Code and Interpret the Results

### Project Structure
The root directory of this project contains the following files and folders:

- **data**: Contains the CSV data provided for the challenge.
- **exploratory_data_analysis**: A Jupyter Notebook used for light data analysis and gaining insights into the dataset.
- **src**: Contains all the code for the challenge, including utility functions, model logic, and main scripts.
- **take_home_challenge_instructions**: A folder containing the PDF instructions for this challenge.
- **README.md**: This file, documenting my thought process, implementation details, and how to run the project.
- **pyproject.toml**: A project configuration file used for managing dependencies and project metadata.
- **uv.lock**: A lock file generated by the `uv` package manager to ensure consistent dependency management.

### Package Management
This project uses the **uv Python package manager** ([GitHub Repository](https://github.com/astral-sh/uv)). If you do not already have `uv` installed, you can install it by following the instructions in the repository.

### Setting Up the Environment

1. **Install `uv`**  
   Ensure you have the `uv` package manager installed. Follow the instructions from the [uv GitHub repository](https://github.com/astral-sh/uv) if you haven't already.

2. **Create a Virtual Environment**  
   In the root directory of the project, run the following command to create a virtual environment:
   ```bash
   uv venv
   ```
3. **Activate the Virtual Environment**  
   Once the virtual environment is created, activate it by running:
   ```bash
   source .venv/bin/activate
   ```

4. **Install Dependencies**  
   Use the following command to install all dependencies specified in the pyproject.toml file:
   ```bash
   uv sync
   ```

After completing these steps, the environment should be fully set up and ready for you to run the code.

### User Input

The challenge requires three inputs from the user:

1. **The Data**  
   - The data is located in the `data` folder and contains the `takehomedataset.csv` file provided as part of this challenge.

2. **User Queries**  
   - In the `playbook/user_input` folder, you will find a file named `user_queries.txt`.  
   - Enter each query on a separate line. Below is an example of what the file might look like:
     ```
     apparel products
     UPS
     maximum days from shipment to delivery
     minimum days from shipment to delivery
     shipment to delivery days less than 5
     estimated arrival date on july 5 2023
     ```

3. **Columns to Include**  
   - In the same `playbook/user_input` folder, there is a file named `df_cols.txt`.  
   - Enter the names of the dataset columns you want to include in this task. Each column name should be on a new line. Below is an example:
     ```
     Priority
     Carrier_name
     Product_Category
     Days_from_shipment_to_delivery
     Estimated_Arrival_Date
     ```

Make sure to fill out these files before running the code. The system uses these inputs to process the queries and generate results.

### Running the Code

Once you have set up everything, you are ready to execute the code. Run the following command in your terminal:

```bash
python src/playbook/runner.py
```

### Logs and Outputs

While the code is running, keep an eye on your terminal for logs being printed. These logs provide real-time updates on the progress of the code execution.

### Outputs

The results of the execution are written to two directories:

#### **`src/playbook/outputs/`**
- This directory contains the main output JSON file with the results of your queries.
- The file is named using a format like `YYYYMMDD_HHMMSS_v#.json` for easy identification of runs.

#### **`src/playbook/similarity_calcs/res/`**
- This directory contains intermediate results related to similarity calculations.
- It is useful for debugging or analyzing how query similarities were computed.
- The file is named using a format like `YYYYMMDD_HHMMSS_v#.json` for easy identification of runs.
    - The version number `v#` is independent of `src/playbook/outputs/` JSON version. Be mindful when deleting files in one of the output folders and not the other. 

### Interpreting the Results

#### **`outputs/` JSON File**
This file contains the final results for each query. Each entry in the JSON corresponds to a query and includes:
- The best-matching column name.
- The best-matching value.
- Row IDs for all rows in the dataset that match the query.
- The similarity score of the match.
- The user query for reference.

##### Example Output:
```json
[
    {
        "column_name": "Product_Category",
        "value": "Apparel",
        "row_ids": [
            ...
        ],
        "best_score": 0.9661468267440796,
        "user_query": "apparel products"
    },
    {
        "column_name": "Carrier_name",
        "value": "UPS",
        "row_ids": [
            ...
        ],
        "best_score": 1.0,
        "user_query": "UPS"
    }
]
```

#### Field Explanations:

- **`column_name`**: The name of the column in the dataset that best matches the user query.
- **`value`**: The specific value within the `column_name` that matches the query.
- **`row_ids`**: All row IDs in the dataset where the condition `column_name=value` is met. 

These fields fulfill the requirements of the challenge.

- **`best_score`**: The similarity score calculated for the result of that query. I included this field to make the results more interpretable.
- **`user_query`**: The original user query inputted by the user. This was added to ensure the results are traceable back to the input query.

The inclusion of the `best_score` and `user_query` fields provides additional clarity and traceability, making it easier to understand how the results relate to the user query.


#### **`similarity_calcs/res/` JSON File**

This file breaks down the similarity calculations performed for each query. It includes:
- Similarity scores across all processed columns and values.

Use these files to analyze and interpret the query results and understand how the matches were determined.

An example JSON structure is as follows:

```json
{
    "apparel products": {
        "Priority": {
            "original_filename": "Priority",
            "value": "Medium",
            "similarity_score": 0.9340893626213074,
            "value_used_for_comparison": "Medium"
        },
        "Carrier_name": {
            "original_filename": "Carrier_name",
            "value": "BlueWater Shipping",
            "similarity_score": 0.9601719379425049,
            "value_used_for_comparison": "BlueWater Shipping"
        },
        "Product_Category": {
            "original_filename": "Product_Category",
            "value": "Apparel",
            "similarity_score": 0.9661468267440796,
            "value_used_for_comparison": "Apparel"
        },
        "Days_from_shipment_to_delivery": {
            "original_filename": "Days_from_shipment_to_delivery",
            "value": 2,
            "similarity_score": 0.9350993633270264,
            "value_used_for_comparison": "days from shipment to delivery 2"
        },
        "Estimated_Arrival_Date": {
            "original_filename": "Estimated_Arrival_Date",
            "value": "2023-07-05",
            "similarity_score": 0.9144634008407593,
            "value_used_for_comparison": "estimated arrival date on July 5 2023"
        }
    },
    "UPS": {
        "Priority": {
            "original_filename": "Priority",
            "value": "High",
            "similarity_score": 0.9838942289352417,
            "value_used_for_comparison": "High"
        },
        "Carrier_name": {
            "original_filename": "Carrier_name",
            "value": "UPS",
            "similarity_score": 1.0,
            "value_used_for_comparison": "UPS"
        },
        "Product_Category": {
            "original_filename": "Product_Category",
            "value": "Groceries",
            "similarity_score": 0.9827259182929993,
            "value_used_for_comparison": "Groceries"
        },
        "Days_from_shipment_to_delivery": {
            "original_filename": "Days_from_shipment_to_delivery",
            "value": 2,
            "similarity_score": 0.9514382481575012,
            "value_used_for_comparison": "days from shipment to delivery 2"
        },
        "Estimated_Arrival_Date": {
            "original_filename": "Estimated_Arrival_Date",
            "value": "2023-07-05",
            "similarity_score": 0.9332790374755859,
            "value_used_for_comparison": "estimated arrival date on July 5 2023"
        }
    }
}
```

### Explanation of the JSON Structure

For each query, the JSON provides the highest similarity score for each column in the dataset.

Each entry in the JSON includes:
- **`original_filename`**: The original column name from the dataset.
- **`value`**: The value in the column that had the highest similarity score with the query.
- **`similarity_score`**: The calculated similarity score for the match.
- **`value_used_for_comparison`**: The processed value used for comparison. For numeric and date columns, this may include some preprocessing.

If we look at the first object in the list in the JSON example above, for the **"apparel products"** query, we see that the **Product_Category** column had the highest similarity score across all columns for the value **"Apparel"**.


This JSON allows you to see how the query matched against each column and value in the dataset, providing detailed insight into the similarity calculation process.
